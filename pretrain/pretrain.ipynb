{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ic6GPnGQiO9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 27 17:35:09 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN X (Pascal)    Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 27%   46C    P8    11W / 250W |    104MiB / 12188MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro P6000        Off  | 00000000:81:00.0 Off |                  Off |\n",
      "| 26%   49C    P8    10W / 250W |  15264MiB / 24449MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2783      G   /usr/lib/xorg/Xorg                 39MiB |\n",
      "|    0   N/A  N/A      2914      G   /usr/bin/gnome-shell               60MiB |\n",
      "|    1   N/A  N/A      7449      C   /usr/bin/python3                 2587MiB |\n",
      "|    1   N/A  N/A     21289      C   .../envs/crawler/bin/python3    12673MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvidia-smi --query-gpu=name --format=csv,noheader | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sulqhTj6hqvM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ArJE6fM5iK43"
   },
   "outputs": [],
   "source": [
    "#Set the path to the data folder, datafile and output folder and files\n",
    "\n",
    "op_folder_name = 'oct2022'\n",
    "\n",
    "root_folder = '/users/kent/jmaharja/drugAbuse/'\n",
    "output_folder = os.path.abspath(os.path.join(root_folder, 'output/'+ op_folder_name))\n",
    "model_folder = os.path.abspath(os.path.join(output_folder, 'RoBERTaMLM/'))\n",
    "tokenizer_folder = os.path.abspath(os.path.join(output_folder, 'TokRoBERTa/'))\n",
    "\n",
    "datafile= '2020_01_01.csv'\n",
    "testfile= '20161007.csv'\n",
    "outputfile = 'submission.csv'\n",
    "\n",
    "input_folder = os.path.abspath(os.path.join(root_folder, 'input/'))\n",
    "datafile_path = os.path.abspath(os.path.join(input_folder, datafile))\n",
    "testfile_path = os.path.abspath(os.path.join(input_folder, testfile))\n",
    "outputfile_path = os.path.abspath(os.path.join(output_folder, outputfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115630, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df =pd.read_csv(datafile_path,lineterminator='\\n',skipinitialspace=True, usecols= ['text'])\n",
    "train_df.rename(columns={'text':'Tweet'}, inplace=True)\n",
    "train_df = train_df.dropna()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsQ4kX6tocxe"
   },
   "source": [
    "# Train a language model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8PO-Hw_Codin"
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 1        # number of epochs to train (default: 10)\n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lAFOM3JTomsZ"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=8192,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YPAdUoiuophi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  49816064\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uO3ppLxzoiLL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = torch.nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jilxgKrDotpL"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "# Create the tokenizer from a trained one\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zQaO0Cusp_Q8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaKbTc05oxlQ"
   },
   "source": [
    "# Building the training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YQmTRL-ho4GK"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "        \n",
    "        for example in df.values:\n",
    "            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
    "            self.examples += [x.input_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vnkSiecwpSUo"
   },
   "outputs": [],
   "source": [
    "# Create the train and evaluation dataset\n",
    "train_dataset = CustomDataset(train_df['Tweet'], tokenizer)\n",
    "eval_dataset = CustomDataset(val_df['Tweet'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,   54,   56,  265,  979,  675,  543, 3326,   94,   30,  933,  402,\n",
       "        1447,  396, 5212,   18,  933,  402, 1447, 1563,   18,  933,  402, 1447,\n",
       "        7672,  619,  358, 7074,   18,  933,  402, 1447,  795, 1503, 2240,   18,\n",
       "         933,  402,  307,    2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]\n",
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S58QaLqypVUc"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kfqw8btGpX_F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/kent/jmaharja/drugAbuse/output/oct2022/RoBERTaMLM\n"
     ]
    }
   ],
   "source": [
    "#from transformers import Trainer, TrainingArguments\n",
    "print(model_folder)\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    save_steps=8192,\n",
    "    #eval_steps=4096,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpINOR7zpafJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/kent/jmaharja/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/users/kent/jmaharja/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running training *****\n",
      "  Num examples = 1004067\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31378\n",
      "/users/kent/jmaharja/.local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='667' max='31378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  667/31378 04:46 < 3:40:47, 2.32 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo_4v-wbpdcb"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Drug-Abuse-Tweets-onRobert",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
