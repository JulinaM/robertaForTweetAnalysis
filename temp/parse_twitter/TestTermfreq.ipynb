{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to create the collection: Prompt dismissed..\u001b[0m\n",
      "Collecting tweepy\n",
      "  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tweepy) (1.15.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 6.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/anaconda3/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    RT @witnessjudas: the tea is that thank u, nex...\n",
       "1    RT @rosy22600250: #TNTrustsModi\\nEvery Indian ...\n",
       "2                             @thecripprint Nah shawty\n",
       "3    Australian Siblings Are Semi-Identical Twins, ...\n",
       "4           @ZiggyZagz I love them both so m u c h omg\n",
       "5    @Sethinomics @kaul_vivek Is he the same guy wh...\n",
       "6    RT @Fafie_07: We can't be friends if you've ne...\n",
       "7    RT @pwcdanica: As we complete the second full ...\n",
       "8    RT @YUE87050559: @PINKKMARKET Please choose 2w...\n",
       "9    @makowoto @nazuniichan if this is a photo of y...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#Part 1:\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "df = pd.read_json(\"/home/jzhu10/TwitterData/190301EnRaw.json\", orient = 'records', lines = True)\n",
    "# above line will be different depending on where you saved your data, and your file name\n",
    "df.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    RT @witnessjudas: the tea is that thank u, nex...\n",
       "1    RT @rosy22600250: #TNTrustsModi\\nEvery Indian ...\n",
       "2                             @thecripprint Nah shawty\n",
       "3    Australian Siblings Are Semi-Identical Twins, ...\n",
       "4           @ZiggyZagz I love them both so m u c h omg\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "all_tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    \"\"\"Replace URLs found in a text string with nothing \n",
    "    (i.e. it will remove the URL from the string).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['RT witnessjudas the tea is that thank u next and 7 rings went viral because of the lyrics and videos content not because it was musica',\n",
       " 'RT rosy22600250 TNTrustsModiEvery Indian must come forward to end social inequality amp help govt promote an environment of compassion amp',\n",
       " 'thecripprint Nah shawty',\n",
       " 'Australian Siblings Are SemiIdentical Twins Some of the Rarest Humans Ever',\n",
       " 'ZiggyZagz I love them both so m u c h omg']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "all_tweets_no_urls = [remove_url(tweet) for tweet in all_tweets]\n",
    "all_tweets_no_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'witnessjudas',\n",
       " 'the',\n",
       " 'tea',\n",
       " 'is',\n",
       " 'that',\n",
       " 'thank',\n",
       " 'u',\n",
       " 'next',\n",
       " 'and',\n",
       " '7',\n",
       " 'rings',\n",
       " 'went',\n",
       " 'viral',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lyrics',\n",
       " 'and',\n",
       " 'videos',\n",
       " 'content',\n",
       " 'not',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'musica']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#Text Cleanup - Address Case Issues\n",
    "all_tweets_no_urls[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'witnessjudas',\n",
       " 'the',\n",
       " 'tea',\n",
       " 'is',\n",
       " 'that',\n",
       " 'thank',\n",
       " 'u',\n",
       " 'next',\n",
       " 'and',\n",
       " '7',\n",
       " 'rings',\n",
       " 'went',\n",
       " 'viral',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lyrics',\n",
       " 'and',\n",
       " 'videos',\n",
       " 'content',\n",
       " 'not',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'musica']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Split the words from one tweet into unique elements\n",
    "all_tweets_no_urls[0].lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(' ', 10546850),\n",
       " ('e', 5528038),\n",
       " ('a', 4282226),\n",
       " ('o', 3903795),\n",
       " ('t', 3857643),\n",
       " ('i', 3464973),\n",
       " ('n', 3381367),\n",
       " ('s', 3039416),\n",
       " ('r', 2897692),\n",
       " ('h', 2215331),\n",
       " ('l', 2179733),\n",
       " ('d', 1672609),\n",
       " ('u', 1531281),\n",
       " ('m', 1359333),\n",
       " ('y', 1303216)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    " #List of all words across tweets\n",
    "all_words = [item for sublist in all_tweets_no_urls for item in sublist]\n",
    "\n",
    "# Create counter\n",
    "counts_no_urls = collections.Counter(all_words)\n",
    "\n",
    "counts_no_urls.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}